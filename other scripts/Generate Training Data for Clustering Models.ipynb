{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyart\n",
    "import os\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "#from netCDF4 import num2date, date2num\n",
    "#import math\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_vol(myradar,field):\n",
    "    \n",
    "    if filt:\n",
    "        gatefilter = give_gatefilter(myradar)\n",
    "        ex_gates = gatefilter._gate_excluded\n",
    "        valid_returns = myradar.fields[field]['data'][:][~ex_gates].flatten()\n",
    "    else:\n",
    "        valid_returns = myradar.fields[field]['data'][:].flatten()\n",
    "    return valid_returns.data\n",
    "\n",
    "def give_gatefilter(myradar):\n",
    "\n",
    "    try:\n",
    "        myradar.check_field_exists('SNR')\n",
    "    except:\n",
    "        myradar.add_field('SNR',\n",
    "                          pyart.retrieve.calculate_snr_from_reflectivity(myradar,\n",
    "                                                                         refl_field='DBZH',\n",
    "                                                                         snr_field=None,\n",
    "                                                                         toa=25000.0))\n",
    "\n",
    "    # Set gatefilters\n",
    "    gatefilter = pyart.correct.GateFilter(myradar)\n",
    "    if VRADH_inside is not None:\n",
    "        gatefilter.exclude_inside('VRADH',VRADH_inside[0],VRADH_inside[1])\n",
    "    if VRADH_outside is not None:\n",
    "        gatefilter.exclude_outside('VRADH',VRADH_outside[0],VRADH_outside[1])\n",
    "    if snr_cutoff is not None:\n",
    "        gatefilter.exclude_below('SNR',snr_cutoff)\n",
    "    if depseck_size is not None:\n",
    "        gatefilter = pyart.correct.despeckle.despeckle_field(myradar,\n",
    "                                                     'VRADH',\n",
    "                                                     gatefilter=gatefilter,\n",
    "                                                     size = depseck_size)\n",
    "    return gatefilter\n",
    "\n",
    "def extract_field_dict(vol_no): \n",
    "    try:\n",
    "        myradar = pyart.aux_io.read_odim_h5('/'.join([dirr,all_files[vol_no]]), file_field_names=True)\n",
    "    except:\n",
    "        print('Skipped ' + all_files[vol_no] + ', error opening file')\n",
    "        return\n",
    "    \n",
    "    flattened_data = {}\n",
    "    for field in fields_to_extract:\n",
    "        flattened_data[field] = []\n",
    "        flattened_data[field].extend(extract_vol(myradar,field))\n",
    "    \n",
    "    if retrieve_height:\n",
    "        if myradar.scan_type == 'rhi':\n",
    "            x, y, z = myradar.get_gate_x_y_z(0, edges=False)\n",
    "            flattened_data['height'] = z\n",
    "            \n",
    "    if retrieve_time:\n",
    "        flattened_data['height']= np.full_like(flattened_data[0],myradar.time['data'][0])\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "def handle_output(result):\n",
    "    if result is None:\n",
    "        return\n",
    "    else:\n",
    "        store = pd.HDFStore(outloc+eventname+'.h5')\n",
    "        #print(result)\n",
    "        store.append(eventname, result, format='t',  data_columns=True)\n",
    "        store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = True\n",
    "\n",
    "fields_to_extract = ['DBZH','ZDR','RHOHV','WRADH','KDP']\n",
    "\n",
    "filt = True\n",
    "\n",
    "#Exclude values above this SNR value\n",
    "snr_cutoff = 2\n",
    "#Exclude values inside these values\n",
    "VRADH_inside = None #[-0.5,0.5]\n",
    "#Exclude values outside these values\n",
    "VRADH_outside = None #[-10,10]\n",
    "#Apply despeckle filter on VRADH with this minimum no of pixels\n",
    "depseck_size = None #10\n",
    "\n",
    "outloc = './training_data/'\n",
    "\n",
    "RHI = True\n",
    "\n",
    "retrieve_height = True\n",
    "retrieve_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin processing 156 files.\n",
      "Skipped 99_20161205_002528.h5, error opening file\n",
      "Skipped 99_20161205_002527.h5, error opening file\n",
      "Skipped 99_20161205_002530.h5, error opening file\n",
      "Skipped 99_20161205_002533.h5, error opening file\n",
      "Skipped 99_20161205_002529.h5, error opening file\n",
      "Skipped 99_20161205_002532.h5, error opening file\n",
      "Skipped 99_20161205_002526.h5, error opening file\n",
      "Skipped 99_20161205_002531.h5, error opening file\n",
      "Skipped 99_20161205_002535.h5, error opening file\n",
      "Skipped 99_20161205_002534.h5, error opening file\n",
      "Skipped 99_20161205_002539.h5, error opening file\n",
      "Skipped 99_20161205_002536.h5, error opening file\n",
      "Skipped 99_20161205_002537.h5, error opening file\n",
      "Skipped 99_20161205_002540.h5, error opening file\n",
      "Skipped 99_20161205_002541.h5, error opening file\n",
      "Skipped 99_20161205_002542.h5, error opening file\n",
      "Skipped 99_20161205_002543.h5, error opening file\n",
      "Skipped 99_20161205_002544.h5, error opening file\n",
      "Skipped 99_20161205_002546.h5, error opening file\n",
      "Skipped 99_20161205_002548.h5, error opening file\n",
      "Skipped 99_20161205_002547.h5, error opening file\n",
      "Skipped 99_20161205_002549.h5, error opening file\n",
      "Skipped 99_20161205_002551.h5, error opening file\n",
      "Skipped 99_20161205_002550.h5, error opening file\n",
      "Skipped 99_20161205_002552.h5, error opening file\n",
      "Skipped 99_20161205_002554.h5, error opening file\n",
      "Skipped 99_20161205_002553.h5, error opening file\n",
      "Skipped 99_20161205_002556.h5, error opening file\n",
      "Skipped 99_20161205_002555.h5, error opening file\n",
      "Skipped 99_20161205_002557.h5, error opening file\n",
      "Skipped 99_20161205_002558.h5, error opening file\n",
      "Skipped 99_20161205_002559.h5, error opening file\n",
      "Skipped 99_20161205_002600.h5, error opening file\n",
      "Skipped 99_20161205_002601.h5, error opening file\n",
      "Skipped 99_20161205_002602.h5, error opening file\n",
      "Skipped 99_20161205_002604.h5, error opening file\n",
      "Skipped 99_20161205_002603.h5, error opening file\n",
      "Skipped 99_20161205_002605.h5, error opening file\n",
      "Skipped 99_20161205_002606.h5, error opening file\n",
      "Skipped 99_20161205_002607.h5, error opening file\n",
      "Skipped 99_20161205_002608.h5, error opening file\n",
      "Skipped 99_20161205_002610.h5, error opening file\n",
      "Skipped 99_20161205_002609.h5, error opening file\n",
      "Skipped 99_20161205_002612.h5, error opening file\n",
      "Skipped 99_20161205_002614.h5, error opening file\n",
      "Skipped 99_20161205_002613.h5, error opening file\n",
      "Skipped 99_20161205_002611.h5, error opening file\n",
      "Skipped 99_20161205_002615.h5, error opening file\n",
      "Skipped 99_20161205_002617.h5, error opening file\n",
      "Skipped 99_20161205_002616.h5, error opening file\n",
      "Skipped 99_20161205_002619.h5, error opening file\n",
      "Skipped 99_20161205_002620.h5, error opening file\n",
      "Skipped 99_20161205_002618.h5, error opening file\n",
      "Skipped 99_20161205_002621.h5, error opening file\n",
      "Begin processing 143 files.\n",
      "Skipped 99_20161206_074250.h5, error opening file\n",
      "Skipped 99_20161206_074251.h5, error opening file\n",
      "Skipped 99_20161206_074252.h5, error opening file\n",
      "Skipped 99_20161206_074253.h5, error opening file\n",
      "Skipped 99_20161206_074254.h5, error opening file\n",
      "Skipped 99_20161206_074256.h5, error opening file\n",
      "Skipped 99_20161206_074255.h5, error opening file\n",
      "Skipped 99_20161206_074257.h5, error opening file\n",
      "Skipped 99_20161206_074259.h5, error opening file\n",
      "Skipped 99_20161206_074300.h5, error opening file\n",
      "Skipped 99_20161206_074301.h5, error opening file\n",
      "Skipped 99_20161206_074302.h5, error opening file\n",
      "Skipped 99_20161206_074258.h5, error opening file\n",
      "Begin processing 3530 files.\n",
      "Begin processing 653 files.\n",
      "Begin processing 2873 files.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_processes = cpu_count()\n",
    "    \n",
    "    eventname = 'Sedgerly_5th'\n",
    "    dirr = './Cluster_analysis/raw_data/Sedgerly/h5_radar_5th/'\n",
    "\n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    eventname = 'Sedgerly_6th'\n",
    "    dirr = './Cluster_analysis/raw_data/Sedgerly/h5_radar_6th/'\n",
    "    \n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    eventname = 'Mt_Bolton'\n",
    "    dirr = './Cluster_analysis/raw_data/Mt Bolton/radar_hdf/'\n",
    "    \n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    eventname = 'Dereel'\n",
    "    dirr = './Cluster_analysis/raw_data/Dereel/Other RHIS/'\n",
    "    \n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    eventname = 'AspeyWest'\n",
    "    dirr = './Cluster_analysis/raw_data/AspeyBurn/Odim/'\n",
    "    \n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin processing 2 files.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_processes = cpu_count()\n",
    "\n",
    "    eventname = 'Mt_Bolton_height'\n",
    "    dirr = './raw_data/Mt Bolton/radar_hdf/'\n",
    "    \n",
    "    all_files = os.listdir(dirr)\n",
    "    all_files.sort()\n",
    "    \n",
    "    if test:\n",
    "        all_files = [all_files[187],all_files[187]]\n",
    "    \n",
    "    print('Begin processing ' + str(len(all_files)) + ' files.')\n",
    "    pool = Pool(num_processes)\n",
    "    for i in range(len(all_files)):\n",
    "        pool.apply_async(extract_field_dict, (i, ), callback=handle_output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
